{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a standalone Python program or Jupyter notebook for coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, donâ€™t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n",
    "You may import more packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import re\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import naive_bayes\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#seed keras model \n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def preprocess(filename):\n",
    "    clean_content_tokenized = []\n",
    "    tweet_sentiment =[]\n",
    "    tweet_id_list =[]\n",
    "    documents = []\n",
    "    \n",
    "    #with open(filename,'r', encoding=\"utf8\") as f:\n",
    "    with open(filename,'r', encoding=\"utf8\") as f:\n",
    "        for idx,tweet in enumerate(f):\n",
    "            tweet = tweet.split('\\t') #split columns by tab\n",
    "\n",
    "            # store 1st col of tweet as tweet_id\n",
    "            tweet_id = tweet[0]\n",
    "            tweet_id_list.append(tweet_id)\n",
    "            # second col as sentiment (pos/neg/neutral)\n",
    "            sentiment = tweet[1]\n",
    "            tweet_sentiment.append(tweet[1])\n",
    "            # third col as tweet_message\n",
    "            tweet_message = tweet[2]\n",
    "\n",
    "            # @user\n",
    "            content = re.sub(r\"@[A-Za-z0-9_]+\", \" USERNAME \", tweet_message.lower())\n",
    "            # URL link\n",
    "            content = re.sub(r\"http\\S+\", \" URLLINK \", content)\n",
    "            #Happy Faces\n",
    "            content = re.sub(r\"(:\\))|(:\\-\\))|(:d)|(:\\-d)|(:D)|(XD)|(xD)|(;D)|(;\\-D)|(;\\))|(;\\-\\))|(lol)|(LOL)|(LOFL)|(lofl)|(lmfao)|(<3)\", \" HEMOTICON \", content, flags = re.IGNORECASE)\n",
    "            #Sad Faces\n",
    "            content = re.sub(r\"(:\\()|(:\\-\\()\", \" SEMOTICON \", content)\n",
    "            #Remove repeater letter in a word e.g. heeeello to hello\n",
    "            content = re.sub(r\"([A-Za-z])\\1{2,}\", r\"\\1\", content)\n",
    "            #Replace all whitespace characters \n",
    "            content = re.sub(r\"\\s\", \" \", content)\n",
    "            # Replace EOS with END\n",
    "            content = re.sub(r\"(\\.|!|\\?) \", \" END \", content) \n",
    "            # Remove non-alphanumeric characters except spaces\n",
    "            content = re.sub(r\"[^A-Za-z0-9 ]\", \"\", content) \n",
    "            #Remove Pure Digits\n",
    "            content = re.sub(r\"\\bd+\\b\", \"\", content)\n",
    "            #Remove Single Letter words eg \"a\"\n",
    "            content = re.sub(r\"\\b[a-z0-9]\\b\", \"\", content)\n",
    "            #tokenize content\n",
    "            document =  nltk.word_tokenize(content)\n",
    "            clean_content_tokenized.append(document)\n",
    "            \n",
    "            \n",
    "    return clean_content_tokenized, tweet_sentiment, tweet_id_list\n",
    "\n",
    "def treebank_pos(word_tag):\n",
    "    #Return TREEBANK TAG Part-of-speech tag\n",
    "    if word_tag.startswith('V'): #verb\n",
    "        return 'v'\n",
    "    elif word_tag.startswith('N'): #noun \n",
    "        return 'n'\n",
    "    elif  word_tag.startswith('J'): #adjective\n",
    "        return 'a'\n",
    "    elif word_tag.startswith('R'): #adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        #set to noun if none is satistfied \n",
    "        return 'n'\n",
    "    \n",
    "def lemmatize_content (content):\n",
    "    content_lemmatized = []\n",
    "    content_pos=[]\n",
    "    content_clean=[]\n",
    "    \n",
    "    #lemmatize using POS tag\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    #POS tagging\n",
    "    for tweet in content:\n",
    "        #assign POS to each word \n",
    "        temp_pos = nltk.pos_tag(tweet)\n",
    "        content_pos.append(temp_pos)  \n",
    "        \n",
    "    for tweet in content_pos:\n",
    "        #Perform lemmatization on each POS word and return the lemmatized word as list\n",
    "        temp_lem = [ lemmatizer.lemmatize(word[0], pos=treebank_pos(word[1])) for word in tweet]\n",
    "        #Remove stop words\n",
    "        temp_lem = [word for word in temp_lem if word not in stopwords] \n",
    "        content_lemmatized.append(temp_lem) \n",
    "\n",
    "    for tweet in content_lemmatized:\n",
    "        #Concatenate content \n",
    "        temp_concat_content = \" \".join(tweet)\n",
    "        content_clean.append(temp_concat_content)\n",
    "        \n",
    "    return content_clean, content_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter-training-data.txt\n",
      "twitter-test1.txt\n",
      "twitter-test2.txt\n",
      "twitter-test3.txt\n"
     ]
    }
   ],
   "source": [
    "# Load training set, dev set and testing set\n",
    "preprocessed_tweets = {}\n",
    "preprocessed_tokenized_tweets = {}\n",
    "tweetgts = {} \n",
    "tweetids = {}\n",
    "\n",
    "for dataset in ['twitter-training-data.txt'] + testsets:\n",
    "    \n",
    "    preprocessed_tweets[dataset] = [] #preprocessed tweet\n",
    "    preprocessed_tokenized_tweets[dataset] = [] #preprocessed and tokenized tweet \n",
    "    tweetids[dataset] = [] #tweet id\n",
    "    tweetgts[dataset] = [] #tweet sentiment\n",
    "    print(dataset)\n",
    "    \n",
    "    \n",
    "    # write code to read in the datasets here\n",
    "    \n",
    "    #retrieve pickled preprocessed Training set\n",
    "    if dataset == 'twitter-training-data.txt':\n",
    "        \n",
    "        if os.path.isfile('train_preprocess.pkl'):\n",
    "            with open('train_preprocess.pkl', 'rb') as file:\n",
    "                preprocessed_tweets[dataset], preprocessed_tokenized_tweets[dataset],tweetgts[dataset],tweetids[dataset] = pickle.load(file)\n",
    "\n",
    "        else:\n",
    "            clean_content_tokenized,tweetgts[dataset],tweetids[dataset]  =preprocess(dataset)\n",
    "            preprocessed_tweets[dataset], preprocessed_tokenized_tweets[dataset] = lemmatize_content(clean_content_tokenized)\n",
    "                        \n",
    "            pickle_file = open('train_preprocess.pkl', 'wb')    \n",
    "            dump_params = preprocessed_tweets[dataset], preprocessed_tokenized_tweets[dataset],tweetgts[dataset],tweetids[dataset]\n",
    "            pickle.dump(dump_params, pickle_file)\n",
    "            pickle_file.close()\n",
    "            \n",
    "    else:\n",
    "        clean_content_tokenized,tweetgts[dataset],tweetids[dataset]  =preprocess(dataset)\n",
    "        preprocessed_tweets[dataset], preprocessed_tokenized_tweets[dataset] = lemmatize_content(clean_content_tokenized)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Function Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "\n",
    "# word level tf-idf\n",
    "def tfidf_word_level(content_train):\n",
    "    #Vectorize content by word with 2000 max features capping\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\S+', max_features= 2000)\n",
    "    tfidf_vect.fit(content_train)\n",
    "    xtrain_tfidf =  tfidf_vect.transform(content_train)\n",
    "\n",
    "    xtrain_tfidf_np = xtrain_tfidf.todense()\n",
    "    xtrain_tfidf_np = np.array(xtrain_tfidf_np)\n",
    "\n",
    "    #return train set and vectorizer\n",
    "    return (xtrain_tfidf_np, tfidf_vect)\n",
    "\n",
    "# ngram level tf-idf\n",
    "def tfidf_ngram_level(content_train):\n",
    "    #Perform Bigram and Trigram Vectorization with max features of 5000\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\S+', ngram_range=(2,3), max_features=5000) #non whitespace chars\n",
    "    tfidf_vect_ngram.fit(content_train)\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(content_train)\n",
    "    \n",
    "    xtrain_tfidf_ngram_np = xtrain_tfidf_ngram.todense()\n",
    "    xtrain_tfidf_ngram_np= np.array(xtrain_tfidf_ngram_np)\n",
    "    \n",
    "    #return train set and vectorizer\n",
    "    return (xtrain_tfidf_ngram_np, tfidf_vect_ngram)\n",
    "\n",
    "\n",
    "# Sentiment By Lexicon\n",
    "def sentiment_by_lexicon(split_corpus):\n",
    "    pos_num = 0\n",
    "    neg_num = 0\n",
    "    \n",
    "    def open_file(lexicon_file):\n",
    "        with open(lexicon_file,'r') as file:\n",
    "            lexicon_content = set(file.read().splitlines())\n",
    "            return lexicon_content\n",
    "            \n",
    "    positive_words= open_file('positive-words.txt')\n",
    "    negative_words = open_file('negative-words.txt')\n",
    "    \n",
    "    positive_negative= []\n",
    "    for tweet in split_corpus:\n",
    "        matched_pos = len( [word for word in tweet if word in positive_words] )\n",
    "        matched_neg = len( [word for word in tweet if word in negative_words] )\n",
    "        positive_negative.append([matched_pos, matched_neg])\n",
    "        \n",
    "    return np.array(positive_negative)\n",
    "\n",
    "# Sentiment By Emoticon \n",
    "def sentiment_by_emoticon(split_corpus):\n",
    "    happy_emoticon = 0\n",
    "    sad_emoticon = 0\n",
    "    \n",
    "    emoticon_feature= []\n",
    "    for tweet in split_corpus:\n",
    "        happy_emoticon = len( [word for word in tweet if word == 'HEMOTICON'] ) \n",
    "        sad_emoticon = len( [word for word in tweet if word == 'SEMOTICON'] )\n",
    "        \n",
    "        emoticon_feature.append([happy_emoticon, sad_emoticon])\n",
    "        \n",
    "    return np.array(emoticon_feature)\n",
    "\n",
    "#Bag of Words\n",
    "def bag_of_word(content_train):\n",
    "    \n",
    "    #BOW using countvectorizer with max of 500 features\n",
    "    bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=500)\n",
    "    x_bow_train = bow_vectorizer.fit_transform(content_train)\n",
    "    x_bow = np.array(x_bow_train.todense())\n",
    "    \n",
    "     #return train set and vectorizer\n",
    "    return (x_bow, bow_vectorizer)\n",
    "\n",
    "\n",
    "if os.path.isfile('word_2_vec.pkl'):\n",
    "    with open('word_2_vec.pkl', 'rb') as file:\n",
    "        model_w2v = pickle.load(file)\n",
    "else:\n",
    "    pickle_file = open('word_2_vec.pkl', 'wb')    \n",
    "\n",
    "    model_w2v = gensim.models.Word2Vec(preprocessed_tweets['twitter-training-data.txt'],\n",
    "                size=1000, # number of features variables\n",
    "                window=10, # window size\n",
    "                min_count=10, # ignore words with frequency<2                     \n",
    "                sg = 1, # skip-gram model\n",
    "                seed = 34\n",
    "    ) \n",
    "    \n",
    "    pickle.dump(model_w2v, pickle_file)\n",
    "    pickle_file.close()\n",
    "            \n",
    "\n",
    "\n",
    "def word_vector(tokens, size):\n",
    "    \n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        #throw exception if token is not in the vocabaulary\n",
    "        except KeyError: \n",
    "            continue\n",
    "            \n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "def glove_LSTM_model(content_train, Y_train, no_tokens , embedding_dim):\n",
    "\n",
    "    # tokenize input content\n",
    "    tk = Tokenizer(num_words=no_tokens)\n",
    "    tk.fit_on_texts(content_train)\n",
    "    #convert text to sequence\n",
    "    content_seq = tk.texts_to_sequences(content_train)\n",
    "    \n",
    "    #Build Glove Dictionary\n",
    "    glove_file = 'glove.6B.100d.txt'\n",
    "    embedding_dict = {}\n",
    "    with open(glove_file,'r', encoding=\"utf8\") as glove:\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_dict[word] = vector\n",
    "\n",
    "    #zero padding numpy array \n",
    "    embedding_matrix = np.zeros((no_tokens, embedding_dim))\n",
    "    \n",
    "    #retrieve 100 dimensional vector for each word\n",
    "    for word, index in tk.word_index.items():\n",
    "        if index < no_tokens:\n",
    "            vect = embedding_dict.get(word)\n",
    "            if vect is not None:\n",
    "                embedding_matrix[index] = vect\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    #get max train length\n",
    "    max_length = np.max([len(text.split()) for text in content_train])\n",
    "    #pad sequence\n",
    "    content_seq_trunc = pad_sequences(content_seq, maxlen=max_length)\n",
    "    \n",
    "    #encoding output\n",
    "    encoder = LabelEncoder()\n",
    "    y_train_encoded = encoder.fit_transform(Y_train)\n",
    "    y_train_categorical = to_categorical(y_train_encoded)\n",
    "    \n",
    "    #train model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(no_tokens, embedding_dim, weights = [embedding_matrix], trainable=False, input_length=max_length ))\n",
    "    model.add((LSTM(80, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add((LSTM(32)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "    History = model.fit(content_seq_trunc,y_train_categorical,epochs = 10,batch_size=100,validation_split =0.2)\n",
    "    \n",
    "    return tk, max_length, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NaiveBayes\n",
      "twitter-test1.txt (NaiveBayes): 0.565\n",
      "twitter-test2.txt (NaiveBayes): 0.593\n",
      "twitter-test3.txt (NaiveBayes): 0.567\n",
      "Training MaxEnt\n",
      "twitter-test1.txt (MaxEnt): 0.571\n",
      "twitter-test2.txt (MaxEnt): 0.597\n",
      "twitter-test3.txt (MaxEnt): 0.551\n",
      "Training LSTM\n",
      "twitter-test1.txt (LSTM): 0.614\n",
      "twitter-test2.txt (LSTM): 0.641\n",
      "twitter-test3.txt (LSTM): 0.578\n"
     ]
    }
   ],
   "source": [
    "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "for classifier in ['NaiveBayes', 'MaxEnt', 'LSTM']: # You may rename the names of the classifiers to something more descriptive\n",
    "        \n",
    "    #Training Models\n",
    "    trainset = 'twitter-training-data.txt'\n",
    "    labels_to_numbers = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}\n",
    "    labels = [labels_to_numbers[sent] for sent in tweetgts[trainset]]\n",
    "    Y_train = np.array(labels)\n",
    "\n",
    "    if classifier == 'NaiveBayes':\n",
    "        print('Training ' + classifier)\n",
    "\n",
    "        \n",
    "        if os.path.isfile('NB.pkl'):\n",
    "            with open('NB.pkl', 'rb') as file:\n",
    "                model_nb,tfidf_word_transformer,tfidf_ngram_transformer = pickle.load(file)\n",
    "        else:\n",
    "      \n",
    "            #extract features: word level tf-idf, ngram level tf-idf, setiment by lexicon and sentiment by emoticon\n",
    "            (xtrain_tfidf_np, tfidf_word_transformer) = tfidf_word_level(preprocessed_tweets[trainset])        \n",
    "            (xtrain_tfidf_ngram_np, tfidf_ngram_transformer) = tfidf_ngram_level(preprocessed_tweets[trainset])\n",
    "            xtrain_sentiment_lexicon = sentiment_by_lexicon(preprocessed_tokenized_tweets[trainset])\n",
    "            xtrain_sentiment_emoticon = sentiment_by_emoticon(preprocessed_tokenized_tweets[trainset])\n",
    "            #concatenate features\n",
    "            xtrain_concat_features = np.concatenate((xtrain_tfidf_np, xtrain_tfidf_ngram_np,xtrain_sentiment_lexicon, xtrain_sentiment_emoticon), axis=1)\n",
    "\n",
    "            # Train sentiment classifier1\n",
    "            model_nb = naive_bayes.MultinomialNB(alpha = 1, fit_prior = True)\n",
    "            model_nb.fit(xtrain_concat_features,Y_train)\n",
    "            pickle_dump = model_nb,tfidf_word_transformer,tfidf_ngram_transformer\n",
    "            \n",
    "            pickle_file = open('NB.pkl', 'wb')        \n",
    "            pickle.dump(pickle_dump, pickle_file)\n",
    "            pickle_file.close()\n",
    "                \n",
    "    elif classifier == 'MaxEnt':\n",
    "        print('Training ' + classifier)\n",
    "        \n",
    "        if os.path.isfile('ME.pkl'):\n",
    "            with open('ME.pkl', 'rb') as file:\n",
    "                 model_me,tfidf_word_transformer,tfidf_ngram_transformer = pickle.load(file)\n",
    "        else:\n",
    "           # TODO: extract features for training classifier2\n",
    "            #extract features: word level tf-idf, ngram level tf-idf, setiment by lexicon and sentiment by emoticon\n",
    "            (xtrain_tfidf_np, tfidf_word_transformer) = tfidf_word_level(preprocessed_tweets[trainset])        \n",
    "            (xtrain_tfidf_ngram_np, tfidf_ngram_transformer) = tfidf_ngram_level(preprocessed_tweets[trainset])\n",
    "            xtrain_sentiment_lexicon = sentiment_by_lexicon(preprocessed_tokenized_tweets[trainset])\n",
    "            xtrain_sentiment_emoticon = sentiment_by_emoticon(preprocessed_tokenized_tweets[trainset])\n",
    "            #concatenate features\n",
    "            xtrain_concat_features = np.concatenate((xtrain_tfidf_np, xtrain_tfidf_ngram_np,xtrain_sentiment_lexicon, xtrain_sentiment_emoticon), axis=1)\n",
    "\n",
    "            # TODO: train sentiment classifier2\n",
    "            model_me = LogisticRegression()\n",
    "            model_me.fit(xtrain_concat_features, Y_train)\n",
    "            \n",
    "            pickle_file = open('ME.pkl', 'wb')    \n",
    "            pickle_dump = model_me,tfidf_word_transformer,tfidf_ngram_transformer\n",
    "            pickle.dump(pickle_dump, pickle_file)\n",
    "            pickle_file.close()\n",
    "            \n",
    "        \n",
    "    elif classifier == 'LSTM':\n",
    "        print('Training ' + classifier)\n",
    "        \n",
    "        \n",
    "        # TODO: extract features for training classifier3  \n",
    "        # TODO: train sentiment classifier3\n",
    "        no_tokens = 5000  # Max no of tokens\n",
    "        embedding_dim = 100  # Embedding dimensionality\n",
    "\n",
    "        lstm_model_save_file = 'LSTM_model.h5'\n",
    "        lstm_parameters_save_file = 'LSTM.pkl'\n",
    "        \n",
    "        if os.path.isfile(lstm_parameters_save_file) & os.path.isfile(lstm_model_save_file):\n",
    "            with open(lstm_parameters_save_file, 'rb') as file:\n",
    "                #retrieve parameters\n",
    "                tokenizer_lstm, max_length_lstm = pickle.load(file)\n",
    "                #retrieve model\n",
    "                lstm_model = load_model(lstm_model_save_file)\n",
    "        else:\n",
    "            #fit model\n",
    "            tokenizer_lstm, max_length_lstm, lstm_model = glove_LSTM_model(preprocessed_tweets[trainset], Y_train, no_tokens , embedding_dim)\n",
    "            \n",
    "            #save parameters\n",
    "            pickle_file = open(lstm_parameters_save_file, 'wb') \n",
    "            pickle_dump = tokenizer_lstm, max_length_lstm\n",
    "            pickle.dump(pickle_dump, pickle_file)\n",
    "            pickle_file.close()\n",
    "            \n",
    "            #save model\n",
    "            lstm_model.save(lstm_model_save_file)\n",
    "        \n",
    "\n",
    "    # Predition performance of the classifiers\n",
    "    for testset in testsets:\n",
    "        id_preds = {}\n",
    "        \n",
    "        # write the prediction and evaluation code here\n",
    "        if classifier == 'NaiveBayes':\n",
    "            #transform test set corpus into features:word level tf-idf, ngram level tf-idf, setiment by lexicon and sentiment by emoticon\n",
    "            # word level tf-idf\n",
    "            xtest_tfidf =  tfidf_word_transformer.transform(preprocessed_tweets[testset])\n",
    "            xtest_tfidf_np = xtest_tfidf.todense()\n",
    "            xtest_tfidf_np= np.array(xtest_tfidf_np)\n",
    "            \n",
    "            # ngram level tf-idf\n",
    "            xtest_tfidf_ngram =  tfidf_ngram_transformer.transform(preprocessed_tweets[testset])\n",
    "            xtest_tfidf_ngram_np = xtest_tfidf_ngram.todense()\n",
    "            xtest_tfidf_ngram_np= np.array(xtest_tfidf_ngram_np)\n",
    "            \n",
    "            xtest_sentiment_lexicon = sentiment_by_lexicon(preprocessed_tokenized_tweets[testset])\n",
    "            xest_sentiment_emoticon = sentiment_by_emoticon(preprocessed_tokenized_tweets[testset])\n",
    "            \n",
    "            #concatenate features\n",
    "            xtest_concat_features = np.concatenate((xtest_tfidf_np, xtest_tfidf_ngram_np,xtest_sentiment_lexicon,xest_sentiment_emoticon), axis=1)\n",
    "            Y_predicted = model_nb.predict(xtest_concat_features)\n",
    "            \n",
    "            \n",
    "        elif classifier == 'MaxEnt':\n",
    "            #transform test set corpus into features:word level tf-idf, ngram level tf-idf, setiment by lexicon and sentiment by emoticon\n",
    "            # word level tf-idf\n",
    "            xtest_tfidf =  tfidf_word_transformer.transform(preprocessed_tweets[testset])\n",
    "            xtest_tfidf_np = xtest_tfidf.todense()\n",
    "            xtest_tfidf_np= np.array(xtest_tfidf_np)\n",
    "            \n",
    "            # ngram level tf-idf\n",
    "            xtest_tfidf_ngram =  tfidf_ngram_transformer.transform(preprocessed_tweets[testset])\n",
    "            xtest_tfidf_ngram_np = xtest_tfidf_ngram.todense()\n",
    "            xtest_tfidf_ngram_np= np.array(xtest_tfidf_ngram_np)\n",
    "            \n",
    "            xtest_sentiment_lexicon = sentiment_by_lexicon(preprocessed_tokenized_tweets[testset])\n",
    "            \n",
    "            xest_sentiment_emoticon = sentiment_by_emoticon(preprocessed_tokenized_tweets[testset])\n",
    "            \n",
    "            #concatenate features\n",
    "            xtest_concat_features = np.concatenate((xtest_tfidf_np, xtest_tfidf_ngram_np,xtest_sentiment_lexicon,xest_sentiment_emoticon), axis=1)\n",
    "            Y_predicted = model_me.predict(xtest_concat_features)\n",
    "            \n",
    "        elif classifier == 'LSTM':\n",
    "            #tokenize and pad test tweets\n",
    "            xtest_seq = tokenizer_lstm.texts_to_sequences(preprocessed_tweets[testset])\n",
    "            xtest_seq_trunc = pad_sequences(xtest_seq, maxlen=max_length_lstm)\n",
    "            \n",
    "            predict_lstm_categorical = lstm_model.predict(xtest_seq_trunc)\n",
    "            Y_predicted = np.argmax(predict_lstm_categorical, axis=1)\n",
    "           \n",
    "        \n",
    "        #cast predicted values back to labels\n",
    "        numbers_to_labels = {0:\"positive\" ,  1 :\"negative\",  2:\"neutral\"}\n",
    "        labels = [numbers_to_labels[i] for i in Y_predicted]\n",
    "        id_preds = dict(zip(tweetids[testset], labels))\n",
    "        \n",
    "        evaluate(id_preds, testset, classifier)\n",
    "        #confusion(id_preds, testset_path, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
